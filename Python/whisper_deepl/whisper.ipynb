{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic speech transcription using Whisper\n",
    "In this module, we use [Whisper](https://github.com/openai/whisper) from OpenAI to transcribe speech automatically. Whisper is a robust automatic speech recognition (ASR) model that supports 99 different languages (e.g., English, Italian, Dutch, Japanese, Chinese, Spanish, etc).\n",
    "\n",
    "Whisper provides 5 multilingual model sizes as follows:\n",
    "\n",
    "|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
    "|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
    "|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
    "|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
    "| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
    "| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
    "| large  |   1550 M   |        N/A         | `large/large-v2` |    ~10 GB     |       1x       |\n",
    "\n",
    "As you can see, the smaller the model is, the faster computational time is, with less accurate results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues in timestamps accuracy\n",
    "The original whisper models do not correctly capture silences. Rather, they just include sliences in timestamps. <br>\n",
    "For example, if an utterance was produced from 00:00:11.000 to 00:00:15.000 followed by a 2-second silence, the timestamp will be 00:00:11.000 - 00:00:17.000 instead. <br>\n",
    "\n",
    "To solve this, we can use **[whisper-timestamped](https://github.com/linto-ai/whisper-timestamped#python)** for improved timing accuracy, word-level timestamps, and partially recovering disfluencies (e.g., uh) as \"*\"\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and define paths\n",
    "\n",
    "Let's first import whisper-timestamped and other required packages.\n",
    "\n",
    "\n",
    "<font color = \"mandarin\">If you haven't installed the packages, follow the steps below</font>\n",
    "\n",
    "1. Open terminal/anaconda prompt at the folder in which you store this notebook\n",
    "1. Activate your conda environment (e.g., conda activate mld_study_group)\n",
    "1. Run this code: pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import whisper_timestamped as whisper\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "input_folder = \"input\"\n",
    "output_folder = \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions that convert time to the format ELAN can recognize.\n",
    "\n",
    "**For float timestamps**\n",
    "1. convert seconds to minute & second timedelta object: 84.14 -> 0:01.24.14\n",
    "2. convert the timedelta object to time object to enable formatting with strftime(): 0:01.24.14 -> 00:00:01.24.140000\n",
    "3. format t using strftime: 00:00:01.24.140000 -> \"00:00:01.24.140\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_float_to_string(flt_timestamp):\n",
    "    t = timedelta(seconds = flt_timestamp)  #step 1\n",
    "    t = (datetime.min + t).time()           #step 2\n",
    "\n",
    "    format = \"%H:%M:%S.%f\"\n",
    "    t_formatted = t.strftime(format)[:-3]   #step 3 *[:-3] means until the third decimals\n",
    "\n",
    "    return t_formatted\n",
    "\n",
    "\n",
    "def convert_time_string_to_float(str_series):\n",
    "    dt_series = pd.to_timedelta(str_series).dt.total_seconds()\n",
    "    return dt_series\n",
    "\n",
    "# str_series = pd.Series([\"01:23:45.678\", \"02:34:56.789\"])\n",
    "# output:\n",
    "# 0    5025.678\n",
    "# 1    9276.789"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to format the Whisper output for ELAN and export it as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_transcript_as_csv(result, output_filename):\n",
    "\n",
    "    df_transcript = pd.DataFrame([], columns=['id', 'start', 'end', 'text'])\n",
    "    transcript_list = []\n",
    "\n",
    "    # save result[\"segments\"] as segments so that we don't need to type result[\"\"] everytime\n",
    "    segments = result[\"segments\"]\n",
    "    \n",
    "    for segment in segments:\n",
    "        # get the start and end time of the segment\n",
    "        id = segment[\"id\"]\n",
    "        start = segment[\"start\"]\n",
    "        end = segment[\"end\"]\n",
    "\n",
    "        # convert the start and end time to string\n",
    "        start = convert_time_float_to_string(start)\n",
    "        end = convert_time_float_to_string(end)\n",
    "\n",
    "        # get the text of the segment\n",
    "        text = segment[\"text\"]\n",
    "\n",
    "        # create a new row for the dataframe\n",
    "        new_row = {'id': id, 'start': start, 'end': end, 'text': text}\n",
    "\n",
    "        # append the new row to the list\n",
    "        transcript_list.append(new_row)\n",
    "    \n",
    "    # add the list to the df_transcript dataframe\n",
    "    df_transcript = pd.concat([df_transcript, pd.DataFrame(transcript_list)], ignore_index=True)\n",
    "\n",
    "    output_file = os.path.join(output_folder, output_filename)\n",
    "    df_transcript.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model and export the output as csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the list of languages</summary>\n",
    "\n",
    "1.  \"en\": \"english\",\n",
    "1.  \"zh\": \"chinese\",\n",
    "1. \"de\": \"german\",\n",
    "1. \"es\": \"spanish\",\n",
    "1. \"ru\": \"russian\",\n",
    "1. \"ko\": \"korean\",\n",
    "1. \"fr\": \"french\",\n",
    "1. \"ja\": \"japanese\",\n",
    "1. \"pt\": \"portuguese\",\n",
    "1. \"tr\": \"turkish\",\n",
    "1. \"pl\": \"polish\",\n",
    "1. \"ca\": \"catalan\",\n",
    "1. \"nl\": \"dutch\",\n",
    "1. \"ar\": \"arabic\",\n",
    "1. \"sv\": \"swedish\",\n",
    "1. \"it\": \"italian\",\n",
    "1. \"id\": \"indonesian\",\n",
    "1. \"hi\": \"hindi\",\n",
    "1. \"fi\": \"finnish\",\n",
    "1. \"vi\": \"vietnamese\",\n",
    "1. \"he\": \"hebrew\",\n",
    "1. \"uk\": \"ukrainian\",\n",
    "1. \"el\": \"greek\",\n",
    "1. \"ms\": \"malay\",\n",
    "1. \"cs\": \"czech\",\n",
    "1. \"ro\": \"romanian\",\n",
    "1. \"da\": \"danish\",\n",
    "1. \"hu\": \"hungarian\",\n",
    "1. \"ta\": \"tamil\",\n",
    "1. \"no\": \"norwegian\",\n",
    "1. \"th\": \"thai\",\n",
    "1. \"ur\": \"urdu\",\n",
    "1. \"hr\": \"croatian\",\n",
    "1. \"bg\": \"bulgarian\",\n",
    "1. \"lt\": \"lithuanian\",\n",
    "1. \"la\": \"latin\",\n",
    "1. \"mi\": \"maori\",\n",
    "1. \"ml\": \"malayalam\",\n",
    "1. \"cy\": \"welsh\",\n",
    "1. \"sk\": \"slovak\",\n",
    "1. \"te\": \"telugu\",\n",
    "1. \"fa\": \"persian\",\n",
    "1. \"lv\": \"latvian\",\n",
    "1. \"bn\": \"bengali\",\n",
    "1. \"sr\": \"serbian\",\n",
    "1. \"az\": \"azerbaijani\",\n",
    "1. \"sl\": \"slovenian\",\n",
    "1. \"kn\": \"kannada\",\n",
    "1. \"et\": \"estonian\",\n",
    "1. \"mk\": \"macedonian\",\n",
    "1. \"br\": \"breton\",\n",
    "1. \"eu\": \"basque\",\n",
    "1. \"is\": \"icelandic\",\n",
    "1. \"hy\": \"armenian\",\n",
    "1. \"ne\": \"nepali\",\n",
    "1. \"mn\": \"mongolian\",\n",
    "1. \"bs\": \"bosnian\",\n",
    "1. \"kk\": \"kazakh\",\n",
    "1. \"sq\": \"albanian\",\n",
    "1. \"sw\": \"swahili\",\n",
    "1. \"gl\": \"galician\",\n",
    "1. \"mr\": \"marathi\",\n",
    "1. \"pa\": \"punjabi\",\n",
    "1. \"si\": \"sinhala\",\n",
    "1. \"km\": \"khmer\",\n",
    "1. \"sn\": \"shona\",\n",
    "1. \"yo\": \"yoruba\",\n",
    "1. \"so\": \"somali\",\n",
    "1. \"af\": \"afrikaans\",\n",
    "1. \"oc\": \"occitan\",\n",
    "1. \"ka\": \"georgian\",\n",
    "1. \"be\": \"belarusian\",\n",
    "1. \"tg\": \"tajik\",\n",
    "1. \"sd\": \"sindhi\",\n",
    "1. \"gu\": \"gujarati\",\n",
    "1. \"am\": \"amharic\",\n",
    "1. \"yi\": \"yiddish\",\n",
    "1. \"lo\": \"lao\",\n",
    "1. \"uz\": \"uzbek\",\n",
    "1. \"fo\": \"faroese\",\n",
    "1. \"ht\": \"haitian creole\",\n",
    "1. \"ps\": \"pashto\",\n",
    "1. \"tk\": \"turkmen\",\n",
    "1. \"nn\": \"nynorsk\",\n",
    "1. \"mt\": \"maltese\",\n",
    "1. \"sa\": \"sanskrit\",\n",
    "1. \"lb\": \"luxembourgish\",\n",
    "1. \"my\": \"myanmar\",\n",
    "1. \"bo\": \"tibetan\",\n",
    "1. \"tl\": \"tagalog\",\n",
    "1. \"mg\": \"malagasy\",\n",
    "1. \"as\": \"assamese\",\n",
    "1. \"tt\": \"tatar\",\n",
    "1. \"haw\": \"hawaiian\",\n",
    "1. \"ln\": \"lingala\",\n",
    "1. \"ha\": \"hausa\",\n",
    "1. \"ba\": \"bashkir\",\n",
    "1. \"jw\": \"javanese\",\n",
    "1. \"su\": \"sundanese\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salma_hayek_short.mp4\n",
      "salma_hayek_short.csv already exists in the output folder\n",
      "salma_hayek_short.wav\n",
      "salma_hayek_short.csv already exists in the output folder\n"
     ]
    }
   ],
   "source": [
    "model_size = \"base\"\n",
    "language = \"en\"\n",
    "model = whisper.load_model(model_size)\n",
    "\n",
    "# iterate over files in the videos folder & apply whisper model on each videos\n",
    "for filename in os.listdir(input_folder):\n",
    "    file = os.path.join(input_folder, filename)\n",
    "    # check if it is a wav file\n",
    "    if filename.endswith(\".wav\") or filename.endswith(\".mp4\"):\n",
    "        # check if the output file already exists\n",
    "        output_filename = filename.split(\".\")[0] + \".csv\"\n",
    "        if os.path.exists(os.path.join(output_folder, output_filename)):\n",
    "            print(f\"{output_filename} already exists in the output folder\")\n",
    "        else:\n",
    "            #apply whisper model on each file\n",
    "            print(\"Now, whisper is working on \" + filename)\n",
    "            result = whisper.transcribe(\n",
    "                model, file, \n",
    "                language=language, \n",
    "                vad=True,               #default = False\n",
    "                no_speech_threshold=0.01,\n",
    "                beam_size=5, \n",
    "                best_of=5, \n",
    "                temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "                detect_disfluencies=True\n",
    "                )\n",
    "\n",
    "            #export the results as csv\n",
    "            export_transcript_as_csv(result, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Exercise 1: Import transcript to ELAN</font>\n",
    "Let's import the transcript to ELAN. [Here](https://www.mpi.nl/corpus/html/elan/ch04s03s01.html#Sec_Importing_CSV_Tab-delimited_Text_Files)'s official documentation of ELAN for importing csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Exercise 2: Change the model size</font>\n",
    "Change the model_size in the code above to \"large-v2\" and run the Whisper model again. After running the model, answer the following questions:\n",
    "\n",
    "- Is the output more accurate compared to the based model?\n",
    "- How long did it take for the large-v2 model to process a 37 seconds video?\n",
    "- Did audience's voice affect the transcription accuracy?\n",
    "\n",
    "*Make sure to add \"_base\" to the filename in the output folder. This is because whisper model won't run if there's a csv file with the same filename as the input wav file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Exercise 3: Combine segments</font>\n",
    "Did you notice that Whisper cut transcript in the middle of sentence? This is because Whisper wants to keep each line of output short so that when they are shown as subtitles, it doesn't go over the screen.\n",
    "\n",
    "However, in research, this is not what we want. We want each line as a sentence, turn construction unit (TCU), etc. So, let's combine some short segments to make them sentences.\n",
    "\n",
    "One thing to keep in mind is that people don't speak perfectly in natural settings: they produce pauses, disfluencies, etc. To reflect this, let's combine segments only if:\n",
    "\n",
    "1. there's no time difference between the end of the target segment and the start of the next segment (diff = 0), and \n",
    "2. there's no period in the target segment. \n",
    "\n",
    "To illustrate, let's take a look at the following example:\n",
    "\n",
    "| id | start | end | text | diff (sec) |\n",
    "|----|-------|-----|------|------|\n",
    "| 0  |00:00:08.940|00:00:13.500|I see the beautiful wedding cake in a little table|0|\n",
    "| 1  |00:00:13.500|00:00:15.300|with two chairs for the bride and the groom.|0|\n",
    "| 2  |00:00:15.300|00:00:16.780|Instead of the bride and the groom,|0|\n",
    "| 3  |00:00:16.780|00:00:21.420|there is Lupe and Angie sitting perfectly.|4.69|\n",
    "\n",
    "Here, we will combine segment 0 and 1 because the time difference between the end of segment 0 and the start of segment 1 is 0, and segment 0 doesn't have a period in it.<br>\n",
    "As for the segment 1 and 2, although the time difference is 0, we will **not** combine them, as segment 1 has a period.<br>\n",
    "Lastly, we will **not** combine segment 3 and 4 because the time difference isn't 0.\n",
    "\n",
    "Let's write a script to implement this!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to combine segments\n",
    "def combine_segments(file_path):\n",
    "    silence_duration = 0  #in seconds\n",
    "    \n",
    "    new_df = pd.DataFrame([], columns=['start', 'end', 'text', 'diff'])\n",
    "    new_list = []\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['diff'] = convert_string_to_float(df['start'].shift(-1)) - convert_string_to_float(df['end']) # we need to convert from string to dtype: float64 to perform calculation\n",
    "\n",
    "    #initiate variables used in the for loop\n",
    "    count = 0\n",
    "    text = \"\"\n",
    "    start = \"\"\n",
    "    end = \"\"\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isnull(df.loc[index, 'text']):            #if the value of text is null \n",
    "            continue                                    #skip the row  \n",
    "        \n",
    "        #when the difference between next segment's start time - current segment's end time is less than \"silence_duration\" and the text doesn't contain \".\"\n",
    "        if df.loc[index, 'diff'] <= silence_duration and \".\" not in df.loc[index, 'text']:\n",
    "            text += df.loc[index, 'text']               #add the value of text to \"text\" variable\n",
    "            if count == 0:                              #if this is the first segment after a long pause (> silence_duration)\n",
    "                start = df.loc[index, 'start']\n",
    "                count += 1\n",
    "\n",
    "        else:\n",
    "            if start == \"\":\n",
    "                start = df.loc[index, 'start']\n",
    "\n",
    "            text += df.loc[index, 'text']\n",
    "            end = df.loc[index, 'end']\n",
    "            diff = df.loc[index, 'diff']\n",
    "\n",
    "            new_row = {'start': start, 'end': end, 'text': text, 'diff': diff}\n",
    "            new_list.append(new_row)\n",
    "\n",
    "            #reset the variables\n",
    "            count = 0\n",
    "            text = \"\"\n",
    "            start = \"\"\n",
    "            end = \"\"\n",
    "\n",
    "    new_df = pd.concat([new_df, pd.DataFrame(new_list)], ignore_index=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "### Function to convert string time to float\n",
    "def convert_string_to_float(str_series):\n",
    "    dt_series = pd.to_timedelta(str_series).dt.total_seconds()\n",
    "    return dt_series\n",
    "\n",
    "# str_series = pd.Series([\"01:23:45.678\", \"02:34:56.789\"])\n",
    "# output:\n",
    "# 0    5025.678\n",
    "# 1    9276.789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, combining segments for salma_hayek_short.csv\n",
      "Now, combining segments for salma_hayek_short_largev2.csv\n"
     ]
    }
   ],
   "source": [
    "### import the csv file and convert the start and end time to float\n",
    "for filename in os.listdir(output_folder):\n",
    "    file_path = os.path.join(output_folder, filename)\n",
    "    # check the file name doesn't end with \"_merged.csv\"\n",
    "    if filename.endswith(\"_merged.csv\"):\n",
    "        continue #skip the file\n",
    "    elif filename.endswith(\".csv\"):\n",
    "        output_filename = filename.split(\".csv\")[0] + \"_merged.csv\"\n",
    "        output_file = os.path.join(output_folder, output_filename)\n",
    "        #check if the output file already exists\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"{output_filename} already exists in the output folder\")\n",
    "        else:\n",
    "            print(\"Now, combining segments for \" + filename)\n",
    "            output = combine_segments(file_path)\n",
    "            output.to_csv(output_file, index=False)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
